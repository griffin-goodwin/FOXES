{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sunpy.net import Fido, attrs as a\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "\n",
    "# Set up logging for better debugging and progress tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. Create directory to save GOES data\n",
    "SAVE_DIR = Path(\"/home/griffingoodwin/downloads/goes_data\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 2. Download GOES data and save to disk\n",
    "def download_and_save_goes_data(start='2023-07-01', end='2023-08-15', max_workers=4):\n",
    "    \"\"\"\n",
    "    Download GOES X-ray data at 1-minute cadence for a specified time range, including all available satellites.\n",
    "\n",
    "    Parameters:\n",
    "    - start (str): Start date for the query (e.g., '2023-07-01')\n",
    "    - end (str): End date for the query (e.g., '2023-08-15')\n",
    "    - max_workers (int): Number of parallel download threads\n",
    "    \"\"\"\n",
    "    logging.info(f\"Searching GOES X-ray data from {start} to {end} for all satellites at 1-minute cadence...\")\n",
    "\n",
    "    # Query for all GOES satellites with 1-minute averaged XRS data\n",
    "    goes_query = Fido.search(\n",
    "        a.Time(start, end),\n",
    "        a.Instrument('XRS'),\n",
    "        a.Resolution('avg1m')  # 1-minute averaged data\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Found {len(goes_query[0])} GOES files.\")\n",
    "\n",
    "    # Skip if no files found\n",
    "    if len(goes_query[0]) == 0:\n",
    "        logging.warning(\"No files found for the specified query.\")\n",
    "        return []\n",
    "\n",
    "    # Define download function for a single file\n",
    "    def download_file(file_entry, path_template):\n",
    "        try:\n",
    "            result = Fido.fetch(file_entry, path=str(path_template / \"{file}\"))\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to download {file_entry['file']}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    logging.info(f\"Downloading {len(goes_query[0])} files with {max_workers} workers...\")\n",
    "    downloaded_files = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create download tasks for each file\n",
    "        futures = [\n",
    "            executor.submit(download_file, row, SAVE_DIR)\n",
    "            for row in goes_query[0]\n",
    "        ]\n",
    "        # Collect results\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            downloaded_files.extend(result)\n",
    "\n",
    "    logging.info(f\"Saved {len(downloaded_files)} files to {SAVE_DIR}\")\n",
    "    return downloaded_files\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files = download_and_save_goes_data(\n",
    "        start='2023-07-01',\n",
    "        end='2023-08-15',\n",
    "        max_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "SAVE_DIR = Path(\"/home/griffingoodwin/downloads/goes_data\")\n",
    "OUTPUT_DIR = Path(\"/home/griffingoodwin/downloads/goes_combined\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Global lists to store used files\n",
    "used_g16_files = []\n",
    "used_g18_files = []\n",
    "\n",
    "def combine_goes_data():\n",
    "    \"\"\"\n",
    "    Combine GOES-16 and GOES-18 files and track source files used.\n",
    "    \"\"\"\n",
    "    global used_g16_files, used_g18_files\n",
    "\n",
    "    g16_files = sorted(SAVE_DIR.glob(\"*g16*.nc\"))\n",
    "    g18_files = sorted(SAVE_DIR.glob(\"*g18*.nc\"))\n",
    "    logging.info(f\"Found {len(g16_files)} GOES-16 and {len(g18_files)} GOES-18 files\")\n",
    "\n",
    "    def process_files(files, satellite_name, output_file, used_file_list):\n",
    "        datasets = []\n",
    "        combined_meta = {}\n",
    "\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                ds = xr.open_dataset(str(file_path))\n",
    "                datasets.append(ds)\n",
    "                used_file_list.append(file_path)  # Track file used\n",
    "                if not combined_meta:\n",
    "                    combined_meta.update(ds.attrs)\n",
    "                logging.info(f\"Loaded {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not load {file_path.name}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                if 'ds' in locals():\n",
    "                    ds.close()\n",
    "\n",
    "        if not datasets:\n",
    "            logging.warning(f\"No valid datasets for {satellite_name}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            combined_ds = xr.concat(datasets, dim='time').sortby('time')\n",
    "            combined_ds.attrs.update({\n",
    "                'satellite': satellite_name,\n",
    "                'instrument': combined_meta.get('instrument', 'XRS'),\n",
    "                'data_level': combined_meta.get('data_level', 'L2'),\n",
    "                'resolution': combined_meta.get('resolution', '1-minute'),\n",
    "                'source': combined_meta.get('source', 'NOAA'),\n",
    "                'start_time': pd.Timestamp(combined_ds.time.min().values).isoformat(),\n",
    "                'end_time': pd.Timestamp(combined_ds.time.max().values).isoformat(),\n",
    "                'source_files': ','.join([f.name for f in used_file_list])\n",
    "            })\n",
    "\n",
    "            combined_ds.to_netcdf(str(output_file), engine='netcdf4')\n",
    "            logging.info(f\"Saved combined file: {output_file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to write {output_file}: {e}\")\n",
    "        finally:\n",
    "            for ds in datasets:\n",
    "                ds.close()\n",
    "\n",
    "    process_files(g16_files, \"GOES-16\", OUTPUT_DIR / \"combined_g16_avg1m.nc\", used_g16_files)\n",
    "    process_files(g18_files, \"GOES-18\", OUTPUT_DIR / \"combined_g18_avg1m.nc\", used_g18_files)\n",
    "\n",
    "def write_used_file_metadata_to_csv():\n",
    "    \"\"\"\n",
    "    Write metadata for only the files used to generate the combined G16 and G18 files.\n",
    "    \"\"\"\n",
    "    OUTPUT_CSV = OUTPUT_DIR / \"used_source_file_metadata.csv\"\n",
    "    files = used_g16_files + used_g18_files\n",
    "\n",
    "    metadata_list = []\n",
    "    logging.info(f\"Extracting metadata from {len(files)} used files...\")\n",
    "\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            ds = xr.open_dataset(str(file_path))\n",
    "            meta = ds.attrs.copy()\n",
    "\n",
    "            meta['file_name'] = file_path.name\n",
    "            meta['satellite'] = 'GOES-16' if 'g16' in file_path.name.lower() else 'GOES-18' if 'g18' in file_path.name.lower() else 'Unknown'\n",
    "\n",
    "            meta.setdefault('instrument', 'XRS')\n",
    "            meta.setdefault('data_level', 'L2')\n",
    "            meta.setdefault('resolution', '1-minute')\n",
    "            meta.setdefault('source', 'NOAA')\n",
    "            meta.setdefault('start_time', 'Unknown')\n",
    "            meta.setdefault('end_time', 'Unknown')\n",
    "\n",
    "            metadata_list.append(meta)\n",
    "            ds.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load metadata from {file_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not metadata_list:\n",
    "        logging.error(\"No metadata could be extracted.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(metadata_list)\n",
    "\n",
    "    # Define standard columns and dynamically add extras\n",
    "    base_columns = [\n",
    "        'file_name', 'satellite', 'instrument', 'data_level',\n",
    "        'resolution', 'source', 'start_time', 'end_time'\n",
    "    ]\n",
    "    extra_columns = set().union(*metadata_list) - set(base_columns)\n",
    "    columns = base_columns + sorted(extra_columns)\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    df = df[columns]\n",
    "\n",
    "    try:\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        logging.info(f\"Saved used source file metadata to {OUTPUT_CSV}\")\n",
    "        print(\"\\nUsed source file metadata:\")\n",
    "        print(df.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write metadata CSV: {e}\")\n",
    "\n",
    "def save_combined_data_as_csv():\n",
    "    \"\"\"\n",
    "    Save combined GOES-16 and GOES-18 data as CSV files.\n",
    "    \"\"\"\n",
    "    columns_to_interp = [\"xrsb_flux\", \"xrsa_flux\"]\n",
    "    try:\n",
    "        # Load GOES-16 data and save as CSV\n",
    "        combined_g16_path = \"/home/griffingoodwin/downloads/goes_combined/combined_g16_avg1m.nc\"\n",
    "        combined_g18_path = \"/home/griffingoodwin/downloads/goes_combined/combined_g18_avg1m.nc\"\n",
    "\n",
    "\n",
    "        goes16 = xr.open_dataset(combined_g16_path)\n",
    "        goes16_df = goes16.to_dataframe().reset_index()\n",
    "        goes16_csv_path = \"/home/griffingoodwin/downloads/goes_combined/combined_g16_avg1m.csv\"\n",
    "        goes16_df = goes16_df[goes16_df['quad_diode']==0]\n",
    "\n",
    "        goes16_df[columns_to_interp] = goes16_df[columns_to_interp].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        goes16_df.to_csv(goes16_csv_path, index=False)\n",
    "\n",
    "        # Load GOES-18 data and save as CSV\n",
    "        goes18 = xr.open_dataset(combined_g18_path)\n",
    "        goes18_df = goes18.to_dataframe().reset_index()\n",
    "        goes18_csv_path = \"/home/griffingoodwin/downloads/goes_combined/combined_g18_avg1m.csv\"\n",
    "        goes18_df = goes18_df[goes18_df['quad_diode']==0]\n",
    "\n",
    "        goes18_df[columns_to_interp] = goes18_df[columns_to_interp].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        goes18_df.to_csv(goes18_csv_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save combined data as CSV: {e}\")\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    combine_goes_data()\n",
    "    write_used_file_metadata_to_csv()\n",
    "    save_combined_data_as_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sunpy.net import Fido, attrs as a\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "class SXRDownloader:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    def __init__(self, save_dir: str = '/downloads/goes_data', concat_dir: str = '/downloads/goes_combined'):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.concat_dir = Path(concat_dir)\n",
    "        self.concat_dir.mkdir(exist_ok=True)\n",
    "        self.used_g13_files = []\n",
    "        self.used_g14_files = []\n",
    "        self.used_g15_files = []\n",
    "        self.used_g16_files = []\n",
    "        self.used_g17_files = []\n",
    "        self.used_g18_files = []\n",
    "\n",
    "\n",
    "    def download_and_save_goes_data(self, start='2023-07-01', end='2023-08-15', max_workers=4):\n",
    "        \"\"\"\n",
    "        Download GOES X-ray data at 1-minute cadence for a specified time range, including all available satellites.\n",
    "\n",
    "        Parameters:\n",
    "        - start (str): Start date for the query (e.g., '2023-07-01')\n",
    "        - end (str): End date for the query (e.g., '2023-08-15')\n",
    "        - max_workers (int): Number of parallel download threads\n",
    "        \"\"\"\n",
    "        logging.info(f\"Searching GOES X-ray data from {start} to {end} for all satellites at 1-minute cadence...\")\n",
    "\n",
    "        # Query for all GOES satellites with 1-minute averaged XRS data\n",
    "        goes_query = Fido.search(\n",
    "            a.Time(start, end),\n",
    "            a.Instrument('XRS'),\n",
    "            a.Resolution('avg1m')  # 1-minute averaged data\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Found {len(goes_query[0])} GOES files.\")\n",
    "\n",
    "        # Skip if no files found\n",
    "        if len(goes_query[0]) == 0:\n",
    "            logging.warning(\"No files found for the specified query.\")\n",
    "            return []\n",
    "\n",
    "        # Define download function for a single file\n",
    "        def download_file(file_entry, path_template):\n",
    "            try:\n",
    "                fido_result = Fido.fetch(file_entry, path=str(path_template / \"{file}\"))\n",
    "                return fido_result\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to download {file_entry['file']}: {e}\")\n",
    "                return []\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel downloads\n",
    "        logging.info(f\"Downloading {len(goes_query[0])} files with {max_workers} workers...\")\n",
    "        downloaded_files = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Create download tasks for each file\n",
    "            futures = [\n",
    "                executor.submit(download_file, row, self.save_dir)\n",
    "                for row in goes_query[0]\n",
    "            ]\n",
    "            # Collect results\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                downloaded_files.extend(result)\n",
    "\n",
    "        logging.info(f\"Saved {len(downloaded_files)} files to {self.save_dir}\")\n",
    "        return downloaded_files\n",
    "\n",
    "\n",
    "    def combine_goes_data(self, columns_to_interp=[\"xrsb_flux\", \"xrsa_flux\"]):\n",
    "        \"\"\"\n",
    "        Combine GOES-16 and GOES-18 files and track source files used.\n",
    "        Parameters\n",
    "        \"\"\"\n",
    "\n",
    "        g13_files = sorted(self.save_dir.glob(\"*g13*.nc\"))\n",
    "        g14_files = sorted(self.save_dir.glob(\"*g14*.nc\"))\n",
    "        g15_files = sorted(self.save_dir.glob(\"*g15*.nc\"))\n",
    "        g16_files = sorted(self.save_dir.glob(\"*g16*.nc\"))\n",
    "        g17_files = sorted(self.save_dir.glob(\"*g17*.nc\"))\n",
    "        g18_files = sorted(self.save_dir.glob(\"*g18*.nc\"))\n",
    "        logging.info(f\"Found {len(g13_files)} GOES-13 files, {len(g14_files)} GOES-14 files, {len(g15_files)} GOES-15 files, {len(g16_files)} GOES-16 files, {len(g17_files)} GOES-17 files, and {len(g18_files)} GOES-18 files.\")\n",
    "\n",
    "        def process_files(files, satellite_name, output_file, used_file_list):\n",
    "            datasets = []\n",
    "            combined_meta = {}\n",
    "\n",
    "            for file_path in files:\n",
    "                try:\n",
    "                    ds = xr.open_dataset(str(file_path))\n",
    "                    datasets.append(ds)\n",
    "                    used_file_list.append(file_path)  # Track file used\n",
    "                    logging.info(f\"Loaded {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Could not load {file_path.name}: {e}\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    if 'ds' in locals():\n",
    "                        ds.close()\n",
    "\n",
    "            if not datasets:\n",
    "                logging.warning(f\"No valid datasets for {satellite_name}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                combined_ds = xr.concat(datasets, dim='time').sortby('time')\n",
    "                if satellite_name in ['GOES-13', 'GOES-14', 'GOES-15']:\n",
    "                    combined_ds['xrsa_flux'] = combined_ds['xrsa_flux'] / .85\n",
    "                    combined_ds['xrsb_flux'] = combined_ds['xrsb_flux'] / .7\n",
    "                df = combined_ds.to_dataframe().reset_index()\n",
    "                if 'quad_diode' in df.columns:\n",
    "                    df = df[df['quad_diode'] == 0]  # Filter out quad diode data\n",
    "                df['time'] = pd.to_datetime(df['time'])\n",
    "                df.set_index('time', inplace=True)\n",
    "                df_log = np.log10(df[columns_to_interp].replace(0, np.nan))\n",
    "\n",
    "                # Step 3: Interpolate in log space\n",
    "                df_log_interp = df_log.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "\n",
    "                # Step 4: Back-transform to linear space\n",
    "                df[columns_to_interp] = 10 ** df_log_interp\n",
    "\n",
    "                # Add min and max dates to filename\n",
    "                min_date = df.index.min().strftime('%Y%m%d')\n",
    "                max_date = df.index.max().strftime('%Y%m%d')\n",
    "                filename = f\"{str(output_file)}_{min_date}_{max_date}.csv\"\n",
    "                df.to_csv(filename, index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                logging.info(f\"Saved combined file: {output_file}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to write {output_file}: {e}\")\n",
    "            finally:\n",
    "                for ds in datasets:\n",
    "                    ds.close()\n",
    "        if len(g13_files) != 0:\n",
    "\n",
    "            process_files(g13_files, \"GOES-13\", self.concat_dir / \"combined_g13_avg1m\", self.used_g13_files)\n",
    "        if len(g14_files) != 0:\n",
    "            process_files(g14_files, \"GOES-14\", self.concat_dir / \"combined_g14_avg1m\", self.used_g14_files)\n",
    "        if len(g15_files) != 0:\n",
    "            process_files(g15_files, \"GOES-15\", self.concat_dir / \"combined_g15_avg1m\", self.used_g15_files)\n",
    "        if len(g16_files) != 0:\n",
    "            process_files(g16_files, \"GOES-16\", self.concat_dir / \"combined_g16_avg1m\", self.used_g16_files)\n",
    "        if len(g17_files) != 0:\n",
    "            process_files(g17_files, \"GOES-17\", self.concat_dir / \"combined_g17_avg1m\", self.used_g17_files)\n",
    "        if len(g18_files) != 0:\n",
    "            process_files(g18_files, \"GOES-18\", self.concat_dir / \"combined_g18_avg1m\", self.used_g18_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = SXRDownloader('/home/griffingoodwin/downloads/goes_data', '/home/griffingoodwin/downloads/goes_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx.download_and_save_goes_data('2023-07-01', '2023-08-15', max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx.combine_goes_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.read_csv('/home/griffingoodwin/downloads/goes_combined/combined_g14_avg1m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(c['xrsa_flux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(c['xrsa_flux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    " c['xrsa_flux']= c['xrsa_flux']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "c['xrsb_flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "c['xrsa_flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.net import Fido, attrs as a\n",
    "\n",
    "goes_query = Fido.search(\n",
    "    a.Time(\"2017-09-11 05:15:00\", \"2017-09-11 05:45:00\"),\n",
    "    a.Instrument('XRS'),\n",
    "    a.Resolution('avg1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(goes_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Fido.search(a.Time('2012/3/4', '2012/3/6'), a.Instrument.lyra, a.Level.two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
