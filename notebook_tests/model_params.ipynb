{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:19.046608Z",
     "start_time": "2025-08-04T17:18:19.025422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.utilities.model_summary import LayerSummary, ModelSummary\n",
    "from torch.nn import HuberLoss\n",
    "import sys\n",
    "sys.path.append(\"/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/\") # go to parent dir\n",
    "print(sys.path)\n",
    "from flaring.forecasting.models.base_model import BaseModel\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class LinearIrradianceModel(BaseModel):\n",
    "    def __init__(self, d_input, d_output, loss_func=HuberLoss(), lr=1e-4):\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        model = nn.Linear(2 * self.n_channels, self.outSize)\n",
    "        super().__init__(model=model, loss_func=loss_func, lr=lr)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "\n",
    "\n",
    "        # Debug: Print input shape\n",
    "        #print(f\"Input shape to LinearIrradianceModel.forward: {x.shape}\")\n",
    "\n",
    "        # Expect x shape: (batch_size, H, W, C)\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D input tensor (batch_size, H, W, C), got shape {x.shape}\")\n",
    "        if x.shape[-1] != self.n_channels:\n",
    "            raise ValueError(f\"AIA image has {x.shape[-1]} channels, expected {self.n_channels}\")\n",
    "\n",
    "        # Calculate mean and std across spatial dimensions (H,W)\n",
    "        # First permute to (batch_size, C, H, W)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Now calculate mean/std across dimensions 2 and 3 (H,W)\n",
    "        mean_irradiance = torch.mean(x, dim=(2, 3))  # Shape: (batch_size, n_channels)\n",
    "        std_irradiance = torch.std(x, dim=(2, 3))    # Shape: (batch_size, n_channels)\n",
    "\n",
    "        # Debug: Print shapes after mean and std\n",
    "        #print(f\"mean_irradiance shape: {mean_irradiance.shape}, std_irradiance shape: {std_irradiance.shape}\")\n",
    "\n",
    "        input_features = torch.cat((mean_irradiance, std_irradiance), dim=1)  # Shape: (batch_size, 2 * n_channels)\n",
    "        #print(f\"Input features shape to linear layer: {input_features.shape}\")\n",
    "\n",
    "        if input_features.shape[1] != 2 * self.n_channels:\n",
    "            raise ValueError(f\"Expected {2 * self.n_channels} features, got {input_features.shape[1]}\")\n",
    "\n",
    "        return self.model(input_features)\n",
    "\n",
    "class HybridIrradianceModel(BaseModel):\n",
    "    def __init__(self, d_input, d_output, cnn_model='resnet', ln_model=True, ln_params=None, lr=1e-4, cnn_dp=0.75, loss_func=HuberLoss()):\n",
    "        super().__init__(model=None, loss_func=loss_func, lr=lr)\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        self.ln_params = ln_params\n",
    "        self.ln_model = None\n",
    "        if ln_model:\n",
    "            self.ln_model = LinearIrradianceModel(d_input, d_output, loss_func=loss_func, lr=lr)\n",
    "        if self.ln_params is not None and self.ln_model is not None:\n",
    "            self.ln_model.model.weight = nn.Parameter(self.ln_params['weight'])\n",
    "            self.ln_model.model.bias = nn.Parameter(self.ln_params['bias'])\n",
    "        self.cnn_model = None\n",
    "        self.cnn_lambda = 1.\n",
    "        if cnn_model == 'resnet':\n",
    "            #deeper model\n",
    "            self.cnn_model = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(d_input, 64, kernel_size=7, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),  # Add batch normalization\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=7, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.AdaptiveAvgPool2d((2, 2)),\n",
    "                nn.Linear(2048, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(128, d_output),\n",
    "            )\n",
    "\n",
    "        elif cnn_model.startswith('efficientnet'):\n",
    "            raise NotImplementedError(\"EfficientNet requires timm; replace with custom CNN or install timm\")\n",
    "        if self.ln_model is None and self.cnn_model is None:\n",
    "            raise ValueError('Please pass at least one model.')\n",
    "\n",
    "    def forward(self, x, sxr=None, **kwargs):\n",
    "        # If x is a tuple (aia_img, sxr_val), extract the AIA image tensor\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[0]\n",
    "\n",
    "        # Expect x shape: (batch_size, H, W, C)\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D input tensor (batch_size, H, W, C), got shape {x.shape}\")\n",
    "        if x.shape[-1] != self.n_channels:\n",
    "            raise ValueError(f\"AIA image has {x.shape[-1]} channels, expected {self.n_channels}\")\n",
    "\n",
    "        # Convert to (batch_size, C, H, W) for CNN\n",
    "        x_cnn = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.ln_model is not None and self.cnn_model is not None:\n",
    "            # For linear model, keep original (B,H,W,C) format\n",
    "            return self.ln_model(x) + self.cnn_lambda * self.cnn_model(x_cnn)\n",
    "        elif self.ln_model is not None:\n",
    "            return self.ln_model(x)\n",
    "        elif self.cnn_model is not None:\n",
    "            return self.cnn_model(x_cnn)\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def set_train_mode(self, mode):\n",
    "        if mode == 'linear':\n",
    "            self.cnn_lambda = 0\n",
    "            if self.cnn_model: self.cnn_model.eval()\n",
    "            if self.ln_model: self.ln_model.train()\n",
    "        elif mode == 'cnn':\n",
    "            self.cnn_lambda = 0.01\n",
    "            if self.cnn_model: self.cnn_model.train()\n",
    "            if self.ln_model: self.ln_model.eval()\n",
    "        elif mode == 'both':\n",
    "            self.cnn_lambda = 0.01\n",
    "            if self.cnn_model: self.cnn_model.train()\n",
    "            if self.ln_model: self.ln_model.train()\n",
    "        else:\n",
    "            raise NotImplementedError(f'Mode not supported: {mode}')"
   ],
   "id": "5e59724efe1f0f0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/jupyter_debug', '/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev', '/tmp/w4scFYbshp', '/home/griffingoodwin/.pycharm_helpers/pydev', '/home/griffingoodwin/.pycharm_helpers/jupyter_debug', '/opt/conda/envs/Flare_detection/lib/python310.zip', '/opt/conda/envs/Flare_detection/lib/python3.10', '/opt/conda/envs/Flare_detection/lib/python3.10/lib-dynload', '', '/home/griffingoodwin/.local/lib/python3.10/site-packages', '/opt/conda/envs/Flare_detection/lib/python3.10/site-packages', '/tmp/tmpotxgyzav', '/opt/conda/envs/Flare_detection/lib/python3.10/site-packages/setuptools/_vendor', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/']\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:19.806915Z",
     "start_time": "2025-08-04T17:18:19.803626Z"
    }
   },
   "cell_type": "code",
   "source": "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
   "id": "3cf5d6f0882bef27",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:20.276236Z",
     "start_time": "2025-08-04T17:18:20.179264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Model = HybridIrradianceModel(6,1,cnn_dp=.2)\n",
    "\n"
   ],
   "id": "ba75ebc2b0eae142",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:20.572178Z",
     "start_time": "2025-08-04T17:18:20.566990Z"
    }
   },
   "cell_type": "code",
   "source": "Model",
   "id": "d5cc05d47a9994b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridIrradianceModel(\n",
       "  (loss_func): HuberLoss()\n",
       "  (ln_model): LinearIrradianceModel(\n",
       "    (model): Linear(in_features=12, out_features=1, bias=True)\n",
       "    (loss_func): HuberLoss()\n",
       "  )\n",
       "  (cnn_model): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU()\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (29): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Dropout(p=0.2, inplace=False)\n",
       "    (32): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Dropout(p=0.2, inplace=False)\n",
       "    (35): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (36): ReLU()\n",
       "    (37): Dropout(p=0.2, inplace=False)\n",
       "    (38): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (39): ReLU()\n",
       "    (40): Dropout(p=0.2, inplace=False)\n",
       "    (41): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (42): ReLU()\n",
       "    (43): Dropout(p=0.2, inplace=False)\n",
       "    (44): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:21.231807Z",
     "start_time": "2025-08-04T17:18:21.225808Z"
    }
   },
   "cell_type": "code",
   "source": "ModelSummary(Model)",
   "id": "501891fbdce3486c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name      | Type                  | Params | Mode \n",
       "------------------------------------------------------------\n",
       "0 | loss_func | HuberLoss             | 0      | train\n",
       "1 | ln_model  | LinearIrradianceModel | 13     | train\n",
       "2 | cnn_model | Sequential            | 12.2 M | train\n",
       "------------------------------------------------------------\n",
       "12.2 M    Trainable params\n",
       "0         Non-trainable params\n",
       "12.2 M    Total params\n",
       "48.988    Total estimated model params size (MB)\n",
       "49        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T16:45:41.759288Z",
     "start_time": "2025-08-04T16:45:41.755632Z"
    }
   },
   "cell_type": "code",
   "source": "import flaring.forecasting.models.vision_transformer_custom as vit_custom",
   "id": "88614e4b77d6cc42",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:29:44.317805Z",
     "start_time": "2025-08-04T15:29:44.313791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kwarg = {\n",
    "    'embed_dim': 512,\n",
    "    'num_channels': 6,\n",
    "    'num_classes': 1,\n",
    "    'patch_size': 16,\n",
    "    'num_patches': 1024,\n",
    "    'hidden_dim': 1024,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 0.0001}"
   ],
   "id": "b4e650ec7ec7727b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:30:12.462378Z",
     "start_time": "2025-08-04T15:30:12.324639Z"
    }
   },
   "cell_type": "code",
   "source": "v = vit_custom.ViT(kwarg, sxr_norm=None)",
   "id": "b3e09333e878cb5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:30:17.609063Z",
     "start_time": "2025-08-04T15:30:17.601677Z"
    }
   },
   "cell_type": "code",
   "source": "ModelSummary(v)",
   "id": "d0cf71415eb0b087",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name  | Type              | Params | Mode \n",
       "----------------------------------------------------\n",
       "0 | model | VisionTransformer | 13.9 M | train\n",
       "----------------------------------------------------\n",
       "13.9 M    Trainable params\n",
       "0         Non-trainable params\n",
       "13.9 M    Total params\n",
       "55.722    Total estimated model params size (MB)\n",
       "73        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:00:00.071353Z",
     "start_time": "2025-08-04T21:00:00.065971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Editor(ABC):\n",
    "    \"\"\"\n",
    "    Editor class for data processing\n",
    "    \"\"\"\n",
    "    def convert(self, data, **kwargs):\n",
    "        result = self.call(data, **kwargs)\n",
    "        if isinstance(result, tuple):\n",
    "            data, add_kwargs = result\n",
    "            kwargs.update(add_kwargs)\n",
    "        else:\n",
    "            data = result\n",
    "        return data, kwargs\n",
    "    @abstractmethod\n",
    "    def call(self, data, **kwargs):\n",
    "        raise NotImplementedError()"
   ],
   "id": "6e9c74bff0841ed3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:07:45.389033Z",
     "start_time": "2025-08-04T21:07:45.382745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from sunpy.map import Map\n",
    "\n",
    "import collections.abc\n",
    "# hyper needs the four following aliases to be done manually.\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.MutableMapping = collections.abc.MutableMapping\n",
    "\n",
    "\n",
    "class EUIPrepEditor(Editor):\n",
    "    \"\"\"\n",
    "    Solar Orbiter EUI data preparation editor to check for missing blocks and invalid resolution\n",
    "    Args:\n",
    "        s_map (sunpy.map.Map): SunPy Map object\n",
    "    Returns:\n",
    "        s_map (sunpy.map.Map): SunPy Map object\n",
    "    \"\"\"\n",
    "    def __init__(self, degradation=None):\n",
    "        self.degradation_fit = np.poly1d(degradation) if degradation else False\n",
    "    def call(self, s_map, **kwargs):\n",
    "        assert np.all(np.logical_not(np.isnan(s_map.data))), 'Found missing block %s' % s_map.date.datetime.isoformat()\n",
    "        assert s_map.meta['NAXIS1'] == 3040 and s_map.meta[\n",
    "            'NAXIS2'] == 3072, 'Found invalid resolution: %s' % s_map.date.datetime.isoformat()\n",
    "        if self.degradation_fit:\n",
    "            x = mdates.date2num(s_map.date.datetime)\n",
    "            correction = self.degradation_fit(x)\n",
    "            s_map = Map(s_map.data / correction, s_map.meta)\n",
    "        return s_map"
   ],
   "id": "ae212cc325daa2ac",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:14:12.379186Z",
     "start_time": "2025-08-04T21:14:12.372366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "from astropy.visualization import ImageNormalize, AsinhStretch\n",
    "from itipy.data.editor import LoadMapEditor, NormalizeRadiusEditor, MapToDataEditor, NormalizeEditor\n",
    "\n",
    "\n",
    "\n",
    "solo_norm = {'eui-fsi174-image': ImageNormalize(vmin=0, vmax=9200, stretch=AsinhStretch(0.005), clip=True),\n",
    "             'eui-fsi304-image': ImageNormalize(vmin=0, vmax=9000, stretch=AsinhStretch(0.001), clip=True) }\n",
    "def getFSIData(f, norm=None):\n",
    "    s_map, path = LoadMapEditor().call(f)\n",
    "    #s_map = EUIPrepEditor(degradation=degradation).call(s_map)\n",
    "    s_map = NormalizeRadiusEditor(resolution=512).call(s_map)\n",
    "    data = MapToDataEditor().call(s_map)\n",
    "    print(data.shape)\n",
    "    data = NormalizeEditor(norm).call(data)\n",
    "    return data\n",
    "\n",
    "getFSIData_174 = partial(getFSIData, norm=solo_norm['eui-fsi174-image'])\n",
    "getFSIData_304 = partial(getFSIData, norm=solo_norm['eui-fsi304-image'])\n"
   ],
   "id": "540637c7a50f0ddf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:17:03.213368Z",
     "start_time": "2025-08-04T21:17:03.208846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itipy.data.dataset import FSIDataset\n",
    "from itipy.data.dataset import get_intersecting_files\n",
    "fsi_files_test_2024 = get_intersecting_files('/mnt/data/ML-Ready_clean/SolO/SolO',\n",
    "                                             ['iti171', 'iti304'])\n",
    "Fsi174 = FSIDataset(fsi_files_test_2024[0], wavelength=\"eui-fsi174-image\", resolution=512)\n",
    "Fsi304 = FSIDataset(fsi_files_test_2024[1], wavelength=\"eui-fsi304-image\", resolution=512)"
   ],
   "id": "a32f06f97757fb59",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:20:20.993516Z",
     "start_time": "2025-08-04T21:20:20.986586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itipy.data.dataset import StackDataset\n",
    "from itipy.data.editor import BrightestPixelPatchEditor\n",
    "\n",
    "\n",
    "class EUIDataset(StackDataset):\n",
    "    def __init__(self, data, patch_shape=None, resolution=1024, **kwargs):\n",
    "        if isinstance(data, list):\n",
    "            paths = data\n",
    "        else:\n",
    "            paths = get_intersecting_files(data, ['eui-fsi174-image', 'eui-fsi304-image'], **kwargs)\n",
    "        data_sets = [FSIDataset(paths[0], 'eui-fsi174-image', resolution=resolution, degradation=[-1.96396999e-04, 4.73016820e+00]),\n",
    "                     FSIDataset(paths[1], 'eui-fsi304-image', resolution=resolution, degradation=[-4.74370815e-04, 1.00064043e+01]),\n",
    "                     #FSIDataset(paths[1], 'eui-fsi304-image', resolution=resolution),\n",
    "                     ]\n",
    "        super().__init__(data_sets, **kwargs)\n",
    "        if patch_shape is not None:\n",
    "            self.addEditor(BrightestPixelPatchEditor(patch_shape))"
   ],
   "id": "67489a38c4b5ce58",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:21:21.074035Z",
     "start_time": "2025-08-04T21:21:21.068272Z"
    }
   },
   "cell_type": "code",
   "source": "eui = EUIDataset(\"/mnt/data/ML-Ready_clean/SolO/SolO\", resolution=512)",
   "id": "4dd26c117224698e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:22:02.130516Z",
     "start_time": "2025-08-04T21:21:59.221028Z"
    }
   },
   "cell_type": "code",
   "source": "eui[0].shape",
   "id": "470a3b267a59ba6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:26:48.380251Z",
     "start_time": "2025-08-04T21:26:48.375858Z"
    }
   },
   "cell_type": "code",
   "source": "eui.getId(1)",
   "id": "bd79fc1e7c0edd15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-03-08T08:15:00'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:31:32.089091Z",
     "start_time": "2025-08-04T21:28:03.722013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i,data in enumerate(eui):\n",
    "    #print(file)\n",
    "    np.save(f\"/mnt/data/ML-Ready_clean/SolO/SolO/ML-Ready-SolO/{eui.getId(i)}.npy\", data)"
   ],
   "id": "fb511bf82d5ee1e9",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T22:08:49.442349Z",
     "start_time": "2025-08-04T22:08:49.418029Z"
    }
   },
   "cell_type": "code",
   "source": "from matplotlib.pyplot import plt",
   "id": "c53ca19e99c7debf",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plt' from 'matplotlib.pyplot' (/opt/conda/envs/Flare_detection/lib/python3.10/site-packages/matplotlib/pyplot.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m plt\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'plt' from 'matplotlib.pyplot' (/opt/conda/envs/Flare_detection/lib/python3.10/site-packages/matplotlib/pyplot.py)"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "383f9f5ffa54f57b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
